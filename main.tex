\documentclass[12pt,draftcls,onecolumn]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}


% Title Page
\title{Loss recovery and tail loss performance in Multipath TCP}
\author{KAU}

\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
TCP has two mechanisms for detecting and recovering from packet losses namely Fast Recovery (FR) and Retransmission Timeouts(RTOs). For long flows where there are sufficiently large number of packets 
to be transferred, FR is quicker than RTOs in detecting and recovering packet losses. On the other hand shorter flows which are a majority in the web traffic have to depend on RTOs for packet loss detection 
and recovery incurring more latency. A single packet loss in a short flow may take many RTTs to detect and recover. This scenario is also applicable to the packets at the end of the flow (or tail) in a long 
flow.

TCP Loss Probe (TLP)~\cite{ietftlp} a mechanism that allows flows to detect and recover from tail losses much faster than an RTO, thereby speeding up short transfers. With TLP a packet loss in the middle 
of a packet train as well as at the tail end will now trigger the same fast recovery mechanisms. It assumes other algorithms such as early retransmit~\cite{rfc5827} and FACK threshold based recovery are 
present.

In Multi-path-TCP(MPTCP)~\cite{rfc6824}, a connection can have multiple TCP sub flows using different interfaces on different routes. Packet losses in each sub flow are assumed to be detected and recovered 
in a similar fashion as that of TCP. However, It is not completely clear how the loss recovery happens in the implementation and which sub-flow retransmits the lost packets. If the recovery is handled at the 
meta level, the lost packet may be rescheduled and retransmitted at the available sub flow with lowest RTT. If the recovery is handled at the flow level, the packet may be retransmitted in the same sub-flow. 

Through this set of preliminary experiments, we would like to understand the behavior of MPTCP with and without TLP and ER by monitoring burst completion time, congestion window evolution and timeout 
behavior.

\section{Related Work}\label{relwork}
TBD.
\section{Scope}\label{scope}

We try to understand the loss recovery and retransmission policies in state of the art MPTCP linux implementation version 0.91. As per the specification of Multipath TCP introduced in 
MPTCP features acknowledgements at connection-level as well as subflow-level, in order to provide a robust transport service to the application.


\section{Experimental Setup}\label{exsetup}

Experiments use CORE emulation platform with a simple topology as depicted in Figure~\ref{fig1}.
Client is connected to two wireless interfaces 3G/4G and WLAN. Server is connected to a wired router.
Characteristics of the connection setup and assumptions about the parameters are provided in table~\ref{tab1}.
There is no attempt in this study to focus on the effect of link characteristics in retransmission performance.
 
\begin{figure}[!ht]
\begin{center}
\includegraphics[angle=0, width=0.9\textwidth]{images/fortest.pdf}
\caption{Topology used for Emulation}\label{fig1}
\end{center}
\end{figure}
\begin{center}

\begin{table}
\begin{center}
\begin{tabular}{|c|cccccccccc|}
      \hline
      \multicolumn{1}{c}{} & & \\[\dimexpr-\normalbaselineskip-\arrayrulewidth]
      \textbf{Burst Size} & \multicolumn{10}{c|}{160 Packets} \\
      \hline
      \textbf{Separation Time} & \multicolumn{10}{c|}{2s} \\
      \hline

      \textbf{RTT} & \multicolumn{5}{c|}{20ms-120ms(3G/4G)} & \multicolumn{5}{c|}{20ms-120ms(WLAN)} \\
      \hline 	
      \textbf{Bandwidth} & \multicolumn{5}{c|}{54Mbps(3G/4G)} & \multicolumn{5}{c|}{54Mbps(WLAN)} \\
      \hline
      \textbf{Loss Model} & \multicolumn{10}{c|}{Deterministic}\\
      \hline
\end{tabular}
\caption{Connection parameters}\label{tab1}
\end{center}
\end{table}
\end{center}


\subsection{Testing with Deterministic loss patterns}
In order to understand the retransmission behavior of the Linux MPTCP implementation and to reproduce the observed retransmissions, we use a deterministic drop pattern.
Losses generated by using associating netem with corresponding interfaces and dropping the packets. This process is simplified by using the KAUNetem tool~\cite{Garcia2016}. 
MPTCP connection starts with a single TCP subflow and subsequently, one or more subflows are added following an agreement between client and server on the MPTCP protocol 
support and interface availability. So one has to wait in time and packets for the second subflow establishment to understand the MPTCP retransmissions. We send two bursts of 80 
packets each and drop tail packets on one of the interface. This is to ensure that the necessary and sufficient conditions for probe triggering are met and tail loss probe is generated on that
subflow. Three test cases are evaluated with one packet tail loss, two packet tail loss and  one packet along with probe loss. 

In Linux, TCP retransmission features are controlled by a sysctl setting tcp.early.retrans. It has 5 possible values with ranging from 0 to 4 with 3 being Linux default, enables both
ER and TLP. For this analysis, we used 0,3 and 4. In each setting, we calculate burst completion time for two burst of 80 packets each. The setup has server and client with MPTCP
enabled linux running on them. Client has two wireless interfaces with one way delay on each interface ranging from 20ms to 120ms. We consider 5 scenarios with delay pairings 20ms-20ms
20ms-30ms, 20ms-120ms, 30ms-20ms, 120ms-20ms to understand the effect of delay difference in the performance. The expected retransmission behavior for different settings of ER is shown 
in~\ref{timingER0},~/\ref{timingER3} and ~\ref{timingER4}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[angle=0, width=0.9\textwidth, natwidth=610, natheight=400]{images/timingER0.pdf}
\end{center}
\caption{Timing diagram of different tail loss scenarios in sysctl ER=0}\label{timingER0}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\includegraphics[angle=0, width=0.9\textwidth, natwidth=610, natheight=400]{images/timingER3.pdf}
\end{center}
\caption{Timing diagram of different tail loss scenarios in sysctl ER=3}\label{timingER3}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\includegraphics[angle=0, width=0.9\textwidth, natwidth=610, natheight=400]{images/timingER4.pdf}
\end{center}
\caption{Timing diagram of different tail loss scenarios in sysctl ER=4}\label{timingER4}
\end{figure}






\section{Observations and Discussion}\label{disc}

Experiments performed on CORE emulator with the setup mentioned in section\ref{exsetup}.
MPTCP is sensitive to path asymmetry in general due to the default scheduler being shortest RTT based scheduler. Wireshark and tcptrace
analysis provides information on the TCP retransmissions at the meta level. mptcptrace provides information on flow sequence split at 
subflow level. Subflow level retransmissions are difficult to test from the output of mptcptrace analysis. Further deep packet inspection
required to understand the losses at the subflow level. 


Figure~\ref{1p} provides burst completion time comparison of different ER sysctl settings considered with one packet loss. The results show the improvements
in burst completion time with ER and TLP enabled compared to both disabled only when there is delay difference. The retransmission of lost packet follows the behavior 
depicted in Figures~\ref{timingER0,timintER3,timingER4}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[angle=0, width=0.9\textwidth,natwidth=578.16,natheight=433.62]{plots/1P.pdf}
\caption{Single packet tail loss}\label{1p}
\end{center}
\end{figure}

In packet trace inspection, it is easy to check the use of TLP with two packet tail loss. The last packet will be retransmitted first in this case.
With one packet tail loss one has to compare the RTT and last packet retransmission time to check the use of TLP.
To further validate the results with one packet tail loss, we consider two packet tail loss and the results are shown in~\ref{2p}.
In this case, ER=3 setting performs better than the other two settings in all scenarios except when the packets lost on high delay interface. 


\begin{figure}[!ht]
\begin{center}
\includegraphics[angle=0, width=0.9\textwidth,natwidth=578.16,natheight=433.62]{plots/2P.pdf}
\caption{Two packet tail loss}\label{2p}
\end{center}
\end{figure}

To further study the performance of TLP in improving latency, we tried to drop the probe packet along with the last packet. In this case ER=0
setting results much lower burst completion times. 

\begin{figure}[!ht]
\begin{center}
\includegraphics[angle=0, width=0.9\textwidth, natwidth=578.16,natheight=433.62]{plots/1PP.pdf}
\caption{Single packet tail loss together with probe loss}\label{1pp}
\end{center}
\end{figure}



\begin{table}[!ht]
\centering
\caption{Tail loss scenarios with tcp.early.retrans = 3 default}
\label{ret3}
\begin{tabular}{|l|l|l|l|l|}
\hline
 testcase   & Asymmetry   & Metalevel          & Subflowlevel       &  \\\hline
Tail drop 1 & 20ms - 30ms & rexmt in same path & rexmt in same path &  \\\hline
Tail drop 2 & 30ms - 20ms & rexmt in same path & TBC                &  \\\hline
Tail drop 3 & 20ms - 20ms & rexmt in same path & TBC                &  \\ \hline
\end{tabular}
\end{table}





\begin{table}[!ht]
\centering
\caption{Tail loss scenarios with tcp.early.retrans = 0 ER disabled}
\label{ret0}
\begin{tabular}{|l|l|l|l|l|}
\hline
 testcase   & Asymmetry   & Metalevel          & Subflowlevel       &  \\\hline
Tail drop 1 & 20ms - 30ms & rexmt in same path & rexmt in same path &  \\\hline
Tail drop 2 & 30ms - 20ms & rexmt in same path & TBC                &  \\\hline
Tail drop 3 & 20ms - 20ms & rexmt in same path & TBC                & \\ \hline
Tail drop 4 & 20ms - 120ms &  			&		&  \\ \hline
Tail drop 5 & 120ms - 20ms &  			& 		& \\ \hline 
\end{tabular}
\end{table}


\begin{table}[!ht]
\centering
\caption{Tail loss scenarios with tcp.early.retrans = 4 TLP disabled}
\label{ret4}
\begin{tabular}{|l|l|l|l|l|}
\hline
 testcase   & Asymmetry   & Metalevel          & Subflowlevel       &  \\\hline
Tail drop 1 & 20ms - 30ms & rexmt in same path & rexmt in same path &  \\\hline
Tail drop 2 & 30ms - 20ms & rexmt in same path & TBC                &  \\\hline
Tail drop 3 & 20ms - 20ms & rexmt in same path & TBC                & \\ \hline
\end{tabular}
\end{table}


\section{Improvements to TLP}
The goal of Tail Loss Probe(TLP) is to reduce tail latency of short flows. It achieves this by converting retransmission timeouts (RTOs) occuring due to tail losses (losses at end of transactions) into fast recovery. TLP transmits one packet in two round-trips when a connection is in Open state and isn't receiving any ACKs. The transmitted packet, aka loss probe, can be either new or a retransmission. When there is tail loss, the ACK from a loss probe triggers FACK/early-retransmit based fast recovery, thus avoiding a costly retransmission timeout. Our results show that current TLP implementation does not improve the performance in MPTCP and incur more latency.

Current implementation of TLP is at the TCP flow level. In the event of RTO timeout the MPTCP retransmission happens with a reinjection in to scheduler along with sending lost packet on the same path. But in the Event of Probe timeout, the lost packet is being sent on the same path without injecting in to scheduler.  Modifications should reinject the lost packet in to the mptcp scheduler in the event of PTO.

\subsection{Example scenario where MPTCP TLP could incur more latency}

Use a traffic pattern where there is new burst to send before the RTO after a tail loss triggering TLP on a flow.


\section{Proposed changes and expected implications}


\subsection{Implementation Plan}

Things to do:



When PTO fires: in code trigger call on PTO in tcp level
It calls ${tcp\_push\_one()}$ in tcp-output.c and then ${mptcp\_write\_xmit()}$.
As it is reinjection (due to only one case currently in MPTCP), that packet will have path mask
to put it on the same subflow. Here we need to add a case for reinjection due to PTO that should
send the packet to $reinject\_queue$ of ${mptcp\_cb}$ as well.


Main flow packet queue and subflow packet queue: ${meta\_sk}$ refers to the
main mptcp flow.

Loss probe timer used in Linux: ${ICSK\_TIME\_LOSS\_PROBE}$.

\section{Evaluation of proposal}

Initial testing with customized program to send bursts of packets with fixed burst seperation. Adjust the burst seperation to get the desired scenario.





\bibliographystyle{IEEEtran}
\bibliography{mptcp-retrans}
\end{document} 
